import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport mathimport osimport pickleimport timeimport warningswarnings.filterwarnings("ignore")# matplotlib.use('TkAgg',force=True)import torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, TensorDatasetfrom torchdiffeq import odeintfrom DMF import get_params, updatedef visualize_weights(weights):    # Compute the difference matrices    diff1 = weights[0] - weights[1]    diff2 = weights[0] - weights[-1]    diff3 = weights[-2] - weights[-1]    # Get the min and max values across all the differences    vmin = min(diff1.min(), diff2.min(), diff3.min())    vmax = max(diff1.max(), diff2.max(), diff3.max())    # Create subplots    fig, axes = plt.subplots(1, 3, figsize=(15, 5))    # Plot the first heatmap    im1 = axes[0].imshow(diff1, cmap="viridis", interpolation="nearest", vmin=vmin, vmax=vmax)    axes[0].set_title("weights[0] - weights[1]")    # Plot the second heatmap    im2 = axes[1].imshow(diff2, cmap="viridis", interpolation="nearest", vmin=vmin, vmax=vmax)    axes[1].set_title("weights[0] - weights[-1]")    # Plot the third heatmap    im3 = axes[2].imshow(diff3, cmap="viridis", interpolation="nearest", vmin=vmin, vmax=vmax)    axes[2].set_title("weights[-2] - weights[-1]")    # Add a single colorbar for all subplots    fig.colorbar(im1, ax=axes, orientation="horizontal", fraction=0.02, pad=0.1)    # Adjust layout    plt.tight_layout()    plt.show()def make_dmf_state_ds(nr_samples, time_steps, nr_pops, state_var):    ds = torch.Tensor(time_steps, nr_samples, nr_pops)    stims = torch.Tensor(nr_samples, nr_pops)    for i in range(nr_samples):        state_DMF = {            'I': np.zeros(M),            'H': np.zeros(M),            'A': np.zeros(M),            'R': np.zeros(M)        }        # stim_DMF = np.zeros(nr_pops, dtype=np.float32)        # stim_DMF[2:3] += 500.0        stim_DMF = np.array([526.8, 447.78, 1044.64, 773.87, 526.8, 447.78, 526.8, 447.78,                             526.8, 447.78, 1044.64, 773.87, 526.8, 447.78, 526.8, 447.78], dtype=np.float32)        stims[i,:] = torch.tensor(stim_DMF, dtype=torch.float32)        for ts in range(time_steps):            state_DMF = update(state_DMF, params, stim_DMF)            ds[ts, i, :] = torch.tensor(state_DMF[state_var], dtype=torch.float32)  # get the membrane potential    return ds, stimsdef make_dmf_ds(nr_samples, time_steps, nr_pops):    ds = torch.Tensor(time_steps, nr_samples, 4, nr_pops)    stims = torch.Tensor(nr_samples, nr_pops)    for i in range(nr_samples):        state_DMF = {            'I': np.zeros(M),            'H': np.zeros(M),            'A': np.zeros(M),            'R': np.zeros(M)        }        # stim_DMF = np.zeros(nr_pops, dtype=np.float32)        # stim_DMF[2:3] += 500.0        stim_DMF = np.array([526.8, 447.78, 1044.64, 773.87, 526.8, 447.78, 526.8, 447.78,                             526.8, 447.78, 1044.64, 773.87, 526.8, 447.78, 526.8, 447.78], dtype=np.float32)        stims[i,:] = torch.tensor(stim_DMF, dtype=torch.float32)        for ts in range(time_steps):            state_DMF = update(state_DMF, params, stim_DMF)            for state_idx, state_var in enumerate(['I', 'H', 'A', 'R']):                ds[ts, i, state_idx, :] = torch.tensor(state_DMF[state_var], dtype=torch.float32)  # get the membrane potential    return ds, stimsclass ThresholdFiringRate(nn.Module):    def __init__(self):        super(ThresholdFiringRate, self).__init__()        self.a = torch.tensor(params['a'], dtype=torch.float32)    # gain        self.b = torch.tensor(params['b'], dtype=torch.float32)    # threshold        self.d = torch.tensor(params['d'], dtype=torch.float32)    # noise factor    def forward(self, x):        x_nom = self.a * x - self.b        x_activ = x_nom / (1 - torch.exp(-self.d * x_nom))        [0.0001 for i in x_activ if i <= 0.]  # ensure positive values (min = 0.0   Hz)        [500 for i in x_activ if i > 500]  # limit firing rates (max = 500.0 Hz)        return x_activclass TwoColumnODE(nn.Module):    def __init__(self, params, M):        super(TwoColumnODE, self).__init__()        # Weights mask        mask = torch.zeros(size=(M, M), dtype=torch.float32)        mask[:8, 8:] += 1.0        mask[8:, :8] += 1.0        self.mask = mask        # Init the weights as trainable parameter        lateral_weights = torch.Tensor(M, M)        nn.init.kaiming_uniform_(lateral_weights, a=math.sqrt(5))  # random init        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(lateral_weights)        lateral_weights *= self.mask  # set inner connectivity to zero        inner_weights = torch.tensor(params['W'], dtype=torch.float32)  # inner connectivity from DMF code        inner_weights *= 1 - self.mask        self.W = nn.Parameter(inner_weights + lateral_weights)  # add the two weights matrices together        # self.W = nn.Parameter(torch.tensor(params['W'], dtype=torch.float32, requires_grad=True))        self.state = {            'I': torch.zeros(M, dtype=torch.float32),            'H': torch.zeros(M, dtype=torch.float32),            'A': torch.zeros(M, dtype=torch.float32),            'R': torch.zeros(M, dtype=torch.float32)        }        # self.activation = ThresholdFiringRate()        self.dt     = torch.tensor(params['dt'], dtype=torch.float32)        self.tau_s  = torch.tensor(params['tau_s'], dtype=torch.float32)        self.W_bg   = torch.tensor(params['W_bg'], dtype=torch.float32)        self.nu_bg  = torch.tensor(params['nu_bg'], dtype=torch.float32)        self.R_     = torch.tensor(params['R'], dtype=torch.float32)  # not to be confused with state['R']        self.tau_m  = torch.tensor(params['tau_m'], dtype=torch.float32)        self.kappa  = torch.tensor(params['kappa'], dtype=torch.float32)        self.tau_a  = torch.tensor(params['tau_a'], dtype=torch.float32)    # def forward(self, t, membrane_pot, stim):    #    #     # dt removed for ode    #     # Update the current    #     current = self.state['I'].detach() + (-self.state['I'].detach() / self.tau_s)  # self inhibition    #     current = current + (torch.matmul(self.W, self.state['R'].detach()))  # recurrent input    #     current = current + self.W_bg * self.nu_bg  # background input    #     current = current + stim  # external output    #     self.state['I'] = current    #    #     # Update the membrane potential and adaptation    #     membrane_pot = membrane_pot + ((-membrane_pot + self.R_ * self.state['I']) / self.tau_m)    #     self.state['H'] = membrane_pot    #     self.state['A'] = self.state['A'] + ((-self.state['A'] + self.state['R'] * self.kappa) / self.tau_a)    #    #     # Update firing rate    #     self.state['R'] = self.activation(self.state['H'] - self.state['A'])    #    #     return membrane_pot  # return the membrane potential    #    #     # Update the current    #     current = self.state['I'].detach() + self.dt * (-self.state['I'].detach() / self.tau_s)  # self inhibition    #     current = current + self.dt * (torch.matmul(self.W*100, self.state['R'].detach()))  # recurrent input    #     current = current + self.dt * self.W_bg * self.nu_bg  # background input    #     current = current + self.dt * stim  # external output    #     self.state['I'] = current    #    #     # Update the membrane potential and adaptation    #     membrane_pot = membrane_pot + self.dt * ((-membrane_pot + self.R_ * self.state['I']) / self.tau_m)    #     self.state['H'] = membrane_pot    #     self.state['A'] = self.state['A'] + self.dt * ((-self.state['A'] + self.state['R'] * self.kappa) / self.tau_a)    #    #     # Update firing rate    #     self.state['R'] = self.activation(self.state['H'] - self.state['A'])    #    #     return membrane_pot  # return the membrane potential    # Training on the current    def forward(self, t, current, state_, stim):        firing_rate, mem_potential, adaptation = state_['R'].detach(), state_['H'].detach(), state_['A'].detach()        current = current + (-current / self.tau_s)  # self inhibition        current = current + (torch.matmul(self.W, firing_rate))  # recurrent input        current = current + self.W_bg * self.nu_bg  # background input        current = current + stim  # external output        return current        self.state['I'] = current        # Update the membrane potential and adaptation        self.state['H'] = mem_potential + ((-mem_potential + self.R_ * current) / self.tau_m)        self.state['A'] = adaptation + ((-adaptation + firing_rate * self.kappa) / self.tau_a)        return current, mem_potential, adaptation        # Update firing rate        self.state['R'] = self.activation(mem_potential - adaptation)        return current        # # Update the current        # current = current + self.dt * (-current / self.tau_s)  # self inhibition        # current = current + self.dt * (torch.matmul(self.W, self.state['R'].detach()))  # recurrent input        # current = current + self.dt * self.W_bg * self.nu_bg  # background input        # current = current + self.dt * stim  # external output        # self.state['I'] = current        #        # # Update the membrane potential and adaptation        # self.state['H'] = self.state['H'] + self.dt * ((-self.state['H'] + self.R_ * current) / self.tau_m)        # self.state['A'] = self.state['A'] + self.dt * ((-self.state['A'] + self.state['R'] * self.kappa) / self.tau_a)        #        # # Update firing rate        # self.state['R'] = self.activation(self.state['H'] - self.state['A'])        #        # return current  # return the currentclass SuperTwoColumnODE(nn.Module):    def __init__(self, params, M):        super(SuperTwoColumnODE, self).__init__()         #self.layer = TwoColumnODE(params, M)        self.activation = ThresholdFiringRate()        # Weights mask        mask = torch.zeros(size=(M, M), dtype=torch.float32)        mask[:8, 8:] += 1.0        mask[8:, :8] += 1.0        self.mask = mask        # Init the weights as trainable parameter        lateral_weights = torch.Tensor(M, M)        nn.init.kaiming_uniform_(lateral_weights, a=math.sqrt(5))  # random init        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(lateral_weights)        lateral_weights *= self.mask  # set inner connectivity to zero        inner_weights = torch.tensor(params['W'], dtype=torch.float32)  # inner connectivity from DMF code        inner_weights *= 1 - self.mask        self.W = nn.Parameter(inner_weights + lateral_weights)  # add the two weights matrices together        self.dt = torch.tensor(params['dt'], dtype=torch.float32)        self.tau_s = torch.tensor(params['tau_s'], dtype=torch.float32)        self.W_bg = torch.tensor(params['W_bg'], dtype=torch.float32)        self.nu_bg = torch.tensor(params['nu_bg'], dtype=torch.float32)        self.R_ = torch.tensor(params['R'], dtype=torch.float32)  # not to be confused with state['R']        self.tau_m = torch.tensor(params['tau_m'], dtype=torch.float32)        self.kappa = torch.tensor(params['kappa'], dtype=torch.float32)        self.tau_a = torch.tensor(params['tau_a'], dtype=torch.float32)    def forward(self, t, state, stim):        dt = self.dt        I, H, A, R = state[0,:], state[1,:], state[2,:], state[3,:]        # Update current (I)        I = I + dt * (-I / self.tau_s)  # self inhibition        I = I + dt * (torch.matmul(self.W, R))  # recurrent input        I = I + dt * self.W_bg * self.nu_bg  # background input        dI = I + dt * stim  # external output        # Update membrane potential (H) and adaptation (A)        dH = H + dt * (-H + self.R_ * dI) / self.tau_m        dA = A + dt * (-A + R * self.kappa) / self.tau_a        # Update firing rate (R)        dR = self.activation(dH - dA)  # Activation function        return torch.stack([dI, dH, dA, dR])if __name__ == '__main__':    nr_samples = 100    # Prepare params    params = get_params(J_local=0.13, J_lateral=0.172, area='MT')    params['dt'] = 1e-4  # timestep    M = params['M']  # num of populations    # Time    time_steps = 100    time_vec = torch.linspace(0., time_steps * params['dt'], time_steps)    # Get the DMF dataset    ds, stims = make_dmf_ds(nr_samples, time_steps, M)    # NN and optimizer    odemodel = SuperTwoColumnODE(params, M)    optimizer = torch.optim.RMSprop(odemodel.parameters(), lr=0.1, alpha=0.99)    # # Training with the ODE    # for i in range(nr_samples):    #     optimizer.zero_grad()    #    #     stim = stims[i, :]    #     starting_point = ds[0, i, :, :]    #     true = ds[:, i, :, :]    #    #     pred = odeint(lambda t, y: odemodel(t, y, stim), starting_point, time_vec)    #    #     plt.plot(true[:, 0].detach().numpy())    #     plt.plot(pred[:, 0].detach().numpy())    #     plt.show()    #    #     loss = torch.mean(torch.abs(pred - true))    #     print(loss)    #    #     loss.backward()    #     optimizer.step()    #    #     # plt.imshow(odemodel.W.detach().numpy().copy(), cmap="viridis", interpolation="nearest")    #     # plt.show()    #    # plt.plot(true[:, 0, 0].detach().numpy())    # plt.plot(pred[:, 0, 0].detach().numpy())    # plt.show()    # Training with just neural network itself        # When using DMF weights this should give perfect results for the first        # training iteration (check state_var when making ds and forward function)    weights = []    strict_mask = torch.zeros(M,M)    strict_mask[1, 8] += 1.    strict_mask[9, 0] += 1.    for iter in range(nr_samples):        optimizer.zero_grad()        stim = stims[0, :]        input = ds[0, 0, :, :]        results_nn = torch.Tensor(time_steps, 4, M)        for i in range(time_steps):            output = odemodel(time_steps, input, stim)            results_nn[i] = output            input = output  # next input for nn is its current output        # plt.plot(results_nn[:, 0, 0].detach().numpy())        # plt.plot(results_nn[:, 1, 0].detach().numpy())        # plt.plot(results_nn[:, 2, 0].detach().numpy())        # plt.plot(results_nn[:, 3, 0].detach().numpy())        # plt.show()        loss = torch.mean(torch.abs(results_nn - ds[:, 0, :, :]))        print(loss)        loss.backward()        with torch.no_grad():            odemodel.W.grad *= odemodel.mask        optimizer.step()        weights.append(odemodel.W.detach().numpy().copy())        if iter%10 == 0:            plt.plot(results_nn[:, 3, 4].detach().numpy())            plt.plot(ds[:, i, 3, 4])            plt.show()            plt.imshow(odemodel.W.detach().numpy(), cmap="viridis", interpolation="nearest")            plt.show()    visualize_weights(weights)
import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport mathimport osimport pickleimport timeimport warningswarnings.filterwarnings("ignore")# matplotlib.use('TkAgg',force=True)import torchimport torch.nn as nnfrom torch.utils.data import DataLoader, TensorDatasetfrom torchdiffeq import odeintfrom DMF import get_params, update, set_visfrom ode_bifurcation import huber_lossdef vis_results(true, pred, state_var, weights, test_stim, train_loss, test_loss, show=False):    '''    0  1  2  3  4  5  6  7    8  9  10 11 12 13 14 15    '''    if not os.path.exists('results/png'):        os.makedirs('results/png')    fig, axes = plt.subplots(2, 3, figsize=(12, 7))    fig.text(0.2, 0.03, f"Input column 1: {test_stim[2]/25.89:.1f}", ha='center', fontsize=12, fontweight='bold')    fig.text(0.4, 0.03, f"Input column 2: {test_stim[10]/25.89:.1f}", ha='center', fontsize=12, fontweight='bold')    fig.text(0.8, 0.03, f"Validation loss: {test_loss:.4f}", ha='center', fontsize=12, fontweight='bold')    fig.text(0.6, 0.03, f"Training loss: {train_loss:.4f}", ha='center', fontsize=12, fontweight='bold')    axes[0,0].plot(true[:, state_var, 0])    axes[0,0].plot(pred[:, state_var, 0], '--')    axes[0,0].plot(true[:, state_var, 8])    axes[0,0].plot(pred[:, state_var, 8], '--')    axes[0,0].set_title("layer 2/3, excitatory")    # axes[0,0].set_ylim(-1, 15)    axes[1,0].plot(true[:, state_var, 1])    axes[1,0].plot(pred[:, state_var, 1], '--')    axes[1,0].plot(true[:, state_var, 9])    axes[1,0].plot(pred[:, state_var, 9], '--')    axes[1,0].set_title("layer 2/3, inhibitory")    # axes[1,0].set_ylim(-1, 15)    axes[0,1].plot(true[:, state_var, 2])    axes[0,1].plot(pred[:, state_var, 2], '--')    axes[0,1].plot(true[:, state_var, 10])    axes[0,1].plot(pred[:, state_var, 10], '--')    axes[0,1].set_title("layer 4, excitatory")    # axes[0,1].set_ylim(-1, 15)    axes[1,1].plot(true[:, state_var, 3])    axes[1,1].plot(pred[:, state_var, 3], '--')    axes[1,1].plot(true[:, state_var, 11])    axes[1,1].plot(pred[:, state_var, 11], '--')    axes[1,1].set_title("layer 4, inhibitory")    axes[0,2].imshow(weights[-1], cmap="viridis", interpolation="nearest")    axes[0,2].set_title("Current weights")    axes[1,2].imshow(weights[0] - weights[-1], cmap="viridis", interpolation="nearest")    axes[1,2].set_title("Difference weights")    plt.tight_layout(pad=3.0)    plt.savefig('results/png/{:02d}'.format(len(weights)))    if show:        plt.show()def make_dmf_ds(nr_samples, time_steps, nr_pops, ds_file, stims_file):    if os.path.exists(ds_file) and os.path.exists(stims_file):        with open(ds_file, 'rb') as f:            ds = pickle.load(f)        with open(stims_file, 'rb') as f:            stims = pickle.load(f)    else:        ds = torch.Tensor(nr_samples, time_steps, 4, nr_pops)        stims = torch.Tensor(nr_samples, nr_pops)        for i in range(nr_samples):            state_DMF = {'I': np.zeros(M), 'H': np.zeros(M), 'A': np.zeros(M), 'R': np.zeros(M)}            # Random stimulus - both stimuli get a value (0.-5.) added or subtracted from 15.            stim_DMF = np.zeros(M)            rnd_diff = np.random.uniform(0.0, 5.0)            if np.random.choice([True, False]):                nu1, nu2 = 15.0+rnd_diff, 15.0-rnd_diff            else:                nu1, nu2 = 15.0-rnd_diff, 15.0+rnd_diff            stim_DMF = set_vis(stim_DMF, column='H', nu=nu1, params=params)            stim_DMF = set_vis(stim_DMF, column='V', nu=nu2, params=params)            stims[i,:] = torch.tensor(stim_DMF, dtype=torch.float32)            for ts in range(time_steps):                state_DMF = update(state_DMF, params, stim_DMF)                for state_idx, state_var in enumerate(['I', 'H', 'A', 'R']):                    ds[i, ts, state_idx, :] = torch.tensor(state_DMF[state_var], dtype=torch.float32)  # get the membrane potential        with open(ds_file, 'wb') as f:            pickle.dump(ds, f)        with open(stims_file, 'wb') as f:            pickle.dump(stims, f)    return ds, stimsdef test_dmf(test_ds, test_stims, iter, state_var, time_steps, M, odemodel, weights, train_loss):    with torch.no_grad():        test_true = test_ds[iter, :, :, :]        test_stim = test_stims[iter, :]        test_res = torch.Tensor(time_steps, 4, M)        test_input = test_true[0, :, :]        for t in range(time_steps):            test_output = odemodel(t, test_input, test_stim)            test_res[t, :, :] = test_output            test_input = test_output        test_loss = huber_loss(test_res[:, state_var, :], test_true[:, state_var, :])        vis_results(test_true, test_res.detach().numpy(), state_var, weights, test_stim, train_loss, test_loss.item(), show=False)class ThresholdFiringRate(nn.Module):    def __init__(self):        super(ThresholdFiringRate, self).__init__()        self.a = torch.tensor(params['a'], dtype=torch.float32)    # gain        self.b = torch.tensor(params['b'], dtype=torch.float32)    # threshold        self.d = torch.tensor(params['d'], dtype=torch.float32)    # noise factor    def forward(self, x):        x_nom = self.a * x - self.b        x_activ = x_nom / (1 - torch.exp(-self.d * x_nom))        [0.0001 for i in x_activ if i <= 0.]  # ensure positive values (min = 0.0   Hz)        [500 for i in x_activ if i > 500]  # limit firing rates (max = 500.0 Hz)        return x_activclass TwoColumnODE(nn.Module):    def __init__(self, params, M):        super(TwoColumnODE, self).__init__()        self.activation = ThresholdFiringRate()        self.dt = torch.tensor(params['dt'], dtype=torch.float32)        self.tau_s = torch.tensor(params['tau_s'], dtype=torch.float32)        self.W_bg = torch.tensor(params['W_bg'], dtype=torch.float32)        self.nu_bg = torch.tensor(params['nu_bg'], dtype=torch.float32)        self.R_ = torch.tensor(params['R'], dtype=torch.float32)  # not to be confused with state['R']        self.tau_m = torch.tensor(params['tau_m'], dtype=torch.float32)        self.kappa = torch.tensor(params['kappa'], dtype=torch.float32)        self.tau_a = torch.tensor(params['tau_a'], dtype=torch.float32)        # Weights mask        mask = torch.zeros(size=(M, M), dtype=torch.float32)        mask[:8, 8:] += 1.0        mask[8:, :8] += 1.0        self.mask = mask        # Init the weights as trainable parameter        # lateral_weights = torch.Tensor(M, M)        # nn.init.kaiming_uniform_(lateral_weights, a=math.sqrt(5))  # random init        # fan_in, _ = nn.init._calculate_fan_in_and_fan_out(lateral_weights)        mean_W = self.dt * params['W'].mean() / 100.        std_W = self.dt * params['W'].std() / 100.        lateral_weights = torch.normal(mean=mean_W, std=std_W, size=(M, M))        lateral_weights *= self.mask  # set inner connectivity to zero        inner_weights = torch.tensor(self.dt * params['W'], dtype=torch.float32)  # inner connectivity from DMF code        # inner_weights *= 1 - self.mask        self.W = nn.Parameter(inner_weights + lateral_weights).to(            dtype=torch.float32)  # add the two weights matrices together        blep = inner_weights + lateral_weights    def forward(self, t, state, stim):        dt = self.dt  # change to 1.0 (ode) or self.dt (nn)        I, H, A, R = state[0,:], state[1,:], state[2,:], state[3,:]        # Update current (I)        I = I + dt * (-I / self.tau_s)  # self inhibition        I = I + (torch.matmul(self.W, R))  # recurrent input  # REMOVED dt *        I = I + dt * self.W_bg * self.nu_bg  # background input        dI = I + dt * stim  # external output        # Update membrane potential (H) and adaptation (A)        dH = H + dt * (-H + self.R_ * dI) / self.tau_m        dA = A + dt * (-A + R * self.kappa) / self.tau_a        # Update firing rate (R)        dR = self.activation(dH - dA)  # activation function        return torch.stack([dI, dH, dA, dR])if __name__ == '__main__':    nr_samples = 1000    batch_size = 16    ds_file = 'pickled_ds/ds_dmf.pkl'    stims_file = 'pickled_ds/stims_dmf.pkl'    # Prepare params    params = get_params(J_local=0.13, J_lateral=0.172, area='MT')    params['dt'] = 1e-4  # timestep    M = params['M']  # num of populations    # Time    time_steps = 1000    time_vec = torch.linspace(0., time_steps * params['dt'], time_steps)    # Get the DMF dataset    ds, stims = make_dmf_ds(nr_samples, time_steps, M, ds_file, stims_file)    # NN and optimizer    odemodel = TwoColumnODE(params, M)    optimizer = torch.optim.RMSprop(odemodel.parameters(), lr=0.001, alpha=0.99)    # # Training with the ODE    # for i in range(nr_samples):    #     optimizer.zero_grad()    #    #     stim = stims[i, :]    #     starting_point = ds[0, i, :, :]    #     true = ds[:, i, :, :]    #    #     pred = odeint(lambda t, y: odemodel(t, y, stim), starting_point, time_vec, rtol=1e-5, atol=1e-7)    #    #     plt.plot(true[:, 0, 0].detach().numpy())    #     plt.plot(pred[:, 0, 0].detach().numpy())    #     plt.show()    #    #     loss = torch.mean(torch.abs(pred - true))    #     print(loss)    #    #     loss.backward()    #     optimizer.step()    # plt.imshow(odemodel.W.detach().numpy().copy(), cmap="viridis", interpolation="nearest")    # plt.show()    #    # plt.plot(true[:, 0, 0].detach().numpy())    # plt.plot(pred[:, 0, 0].detach().numpy())    # plt.show()    state_var = 1  # 0:I, 1:H, 2:A, 3:R    weights = []    strict_mask = torch.zeros(M,M)    strict_mask[1, 8] += 1.    strict_mask[9, 0] += 1.    split = int(nr_samples*0.9)    train_ds, test_ds = ds[:split, :, :, :], ds[split:, :, :, :]    train_stims, test_stims = stims[:split, :], stims[split:, :]    train_dataset = TensorDataset(train_ds, train_stims)    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)    for iter, (true_state, stim_batch) in enumerate(train_loader):        optimizer.zero_grad()        results_nn = torch.Tensor(len(true_state), time_steps, 4, M)        for batch_iter in range(len(true_state)):            stim = stim_batch[batch_iter]            input_state = true_state[batch_iter, 0, :, :]            for t in range(time_steps):                output_state = odemodel(t, input_state, stim)                results_nn[batch_iter, t, :, :] = output_state                input_state = output_state  # next input for nn is its current output        loss = huber_loss(results_nn[:, :, state_var, :], true_state[:, :, state_var, :])        loss.backward()        with torch.no_grad():            odemodel.W.grad *= strict_mask        optimizer.step()        weights.append(odemodel.W.detach().numpy().copy())        print('Iter {:02d} | Total Loss {:.5f}'.format(iter, loss.item()))        # test after each training batch        test_dmf(test_ds, test_stims, iter, state_var, time_steps, M, odemodel, weights, loss.item())
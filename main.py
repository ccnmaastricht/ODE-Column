import numpy as npimport matplotlibimport matplotlib.pyplot as pltimport mathimport osimport pickleimport timeimport warningswarnings.filterwarnings("ignore")# matplotlib.use('TkAgg',force=True)import torchimport torch.nn as nnfrom torchdiffeq import odeintfrom DMF import get_params, updatefrom ode_bifurcation import huber_lossfrom test import *def vis_results(true, pred, weights, show=False):    '''    0  1  2  3  4  5  6  7    8  9  10 11 12 13 14 15    '''    if not os.path.exists('results/png'):        os.makedirs('results/png')    fig, axes = plt.subplots(2, 3, figsize=(12, 7))    axes[0,0].plot(true[:, 3, 2])    axes[0,0].plot(pred[:, 3, 2], '--')    axes[0,0].set_title("Column 1, layer 4 (ex), firing rates")    axes[0,0].set_ylim(-1, 25)    axes[1,0].plot(true[:, 3, 10])    axes[1,0].plot(pred[:, 3, 10], '--')    axes[1,0].set_title("Column 2, layer 4 (ex), firing rates")    axes[1,0].set_ylim(-1, 25)    axes[0,1].plot(true[:, 3, 2])    axes[0,1].plot(pred[:, 3, 2], '--')    axes[0,1].set_title("Column 1, layer 4 (in), firing rates")    axes[0,1].set_ylim(-1, 25)    axes[1,1].plot(true[:, 3, 10])    axes[1,1].plot(pred[:, 3, 10], '--')    axes[1,1].set_title("Column 2, layer 4 (in), firing rates")    axes[1,1].set_ylim(-1, 25)    axes[0,2].imshow(weights[-1], cmap="viridis", interpolation="nearest")    axes[0,2].set_title("Current weights")    axes[1,2].imshow(weights[0] - weights[-1], cmap="viridis", interpolation="nearest")    axes[1,2].set_title("Difference weights")    plt.tight_layout()    plt.savefig('results/png/{:02d}'.format(len(weights)))    if show:        plt.show()def make_dmf_ds(nr_samples, time_steps, nr_pops):    ds = torch.Tensor(time_steps, nr_samples, 4, nr_pops)    stims = torch.Tensor(nr_samples, nr_pops)    for i in range(nr_samples):        state_DMF = {            'I': np.zeros(M),            'H': np.zeros(M),            'A': np.zeros(M),            'R': np.zeros(M)        }        # stim_DMF = np.zeros(nr_pops, dtype=np.float32)        # stim_DMF[2:3] += 500.0        # stim_DMF = np.array([526.8, 447.78, 1044.64, 773.87, 526.8, 447.78, 526.8, 447.78,        #                      526.8, 447.78, 526.8, 447.78, 526.8, 447.78, 526.8, 447.78], dtype=np.float32)        low, high = 400, 1050  # range        stim_DMF = (low + (high - low) * np.random.rand(16)).astype(np.float32)        stims[i,:] = torch.tensor(stim_DMF, dtype=torch.float32)        for ts in range(time_steps):            state_DMF = update(state_DMF, params, stim_DMF)            for state_idx, state_var in enumerate(['I', 'H', 'A', 'R']):                ds[ts, i, state_idx, :] = torch.tensor(state_DMF[state_var], dtype=torch.float32)  # get the membrane potential    return ds, stimsclass ThresholdFiringRate(nn.Module):    def __init__(self):        super(ThresholdFiringRate, self).__init__()        self.a = torch.tensor(params['a'], dtype=torch.float32)    # gain        self.b = torch.tensor(params['b'], dtype=torch.float32)    # threshold        self.d = torch.tensor(params['d'], dtype=torch.float32)    # noise factor    def forward(self, x):        x_nom = self.a * x - self.b        x_activ = x_nom / (1 - torch.exp(-self.d * x_nom))        [0.0001 for i in x_activ if i <= 0.]  # ensure positive values (min = 0.0   Hz)        [500 for i in x_activ if i > 500]  # limit firing rates (max = 500.0 Hz)        return x_activclass TwoColumnODE(nn.Module):    def __init__(self, params, M):        super(TwoColumnODE, self).__init__()        self.activation = ThresholdFiringRate()        # Weights mask        mask = torch.zeros(size=(M, M), dtype=torch.float32)        mask[:8, 8:] += 1.0        mask[8:, :8] += 1.0        self.mask = mask        # Init the weights as trainable parameter        lateral_weights = torch.Tensor(M, M)        nn.init.kaiming_uniform_(lateral_weights, a=math.sqrt(5))  # random init        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(lateral_weights)        lateral_weights *= self.mask * 100  # set inner connectivity to zero        inner_weights = torch.tensor(params['W'], dtype=torch.float32)  # inner connectivity from DMF code        inner_weights *= 1 - self.mask        self.W = nn.Parameter(inner_weights + lateral_weights).to(dtype=torch.float32)  # add the two weights matrices together        self.dt = torch.tensor(params['dt'], dtype=torch.float32)        self.tau_s = torch.tensor(params['tau_s'], dtype=torch.float32)        self.W_bg = torch.tensor(params['W_bg'], dtype=torch.float32)        self.nu_bg = torch.tensor(params['nu_bg'], dtype=torch.float32)        self.R_ = torch.tensor(params['R'], dtype=torch.float32)  # not to be confused with state['R']        self.tau_m = torch.tensor(params['tau_m'], dtype=torch.float32)        self.kappa = torch.tensor(params['kappa'], dtype=torch.float32)        self.tau_a = torch.tensor(params['tau_a'], dtype=torch.float32)    def forward(self, t, state, stim):        dt = self.dt  # change to 1.0 (ode) or self.dt (nn)        I, H, A, R = state[0,:], state[1,:], state[2,:], state[3,:]        # Update current (I)        I = I + dt * (-I / self.tau_s)  # self inhibition        I = I + dt * (torch.matmul(self.W, R))  # recurrent input        I = I + dt * self.W_bg * self.nu_bg  # background input        dI = I + dt * stim  # external output        # Update membrane potential (H) and adaptation (A)        dH = H + dt * (-H + self.R_ * dI) / self.tau_m        dA = A + dt * (-A + R * self.kappa) / self.tau_a        # Update firing rate (R)        dR = self.activation(dH - dA)  # activation function        return torch.stack([dI, dH, dA, dR])if __name__ == '__main__':    nr_samples = 1000    # Prepare params    params = get_params(J_local=0.13, J_lateral=0.172, area='MT')    params['dt'] = 1e-4  # timestep    M = params['M']  # num of populations    # Time    time_steps = 1000    time_vec = torch.linspace(0., time_steps * params['dt'], time_steps)    # Get the DMF dataset    ds, stims = make_dmf_ds(nr_samples, time_steps, M)    # NN and optimizer    odemodel = TwoColumnODE(params, M)    optimizer = torch.optim.RMSprop(odemodel.parameters(), lr=0.01, alpha=0.9)    # # Training with the ODE    # for i in range(nr_samples):    #     optimizer.zero_grad()    #    #     stim = stims[i, :]    #     starting_point = ds[0, i, :, :]    #     true = ds[:, i, :, :]    #    #     pred = odeint(lambda t, y: odemodel(t, y, stim), starting_point, time_vec, rtol=1e-5, atol=1e-7)    #    #     plt.plot(true[:, 0, 0].detach().numpy())    #     plt.plot(pred[:, 0, 0].detach().numpy())    #     plt.show()    #    #     loss = torch.mean(torch.abs(pred - true))    #     print(loss)    #    #     loss.backward()    #     optimizer.step()    # plt.imshow(odemodel.W.detach().numpy().copy(), cmap="viridis", interpolation="nearest")    # plt.show()    #    # plt.plot(true[:, 0, 0].detach().numpy())    # plt.plot(pred[:, 0, 0].detach().numpy())    # plt.show()    # Training with just neural network itself        # When using DMF weights this should give perfect results for the first        # training iteration (check state_var when making ds and forward function)    weights = []    strict_mask = torch.zeros(M,M)    strict_mask[1, 8] += 1.    strict_mask[9, 0] += 1.    for iter in range(nr_samples):        optimizer.zero_grad()        stim = stims[iter, :]        input = ds[0, iter, :, :]        results_nn = torch.Tensor(time_steps, 4, M)        for i in range(time_steps):            output = odemodel(time_steps, input, stim)            results_nn[i] = output            input = output  # next input for nn is its current output        loss = huber_loss(results_nn, ds[:, 0, :, :]) # torch.mean(torch.abs(results_nn - ds[:, 0, :, :]))        loss.backward()        with torch.no_grad():            odemodel.W.grad *= odemodel.mask        optimizer.step()        weights.append(odemodel.W.detach().numpy().copy())        if iter%10 == 0:            print(loss.item())            vis_results(ds[:, iter, :, :], results_nn.detach().numpy(), weights, show=False)    ### testing bistable perception    # dt = 1e-4    # t_sim = 1000    # T = int(t_sim / dt)    #    # R = np.zeros((M, T))    #    # stim = stims[0, :]    # state = ds[0, 0, :, :]    #    # for t in range(t_sim):    #     new_state = odemodel(T, state, stim)    #    #     R[:, t] = new_state[3,:].detach().numpy()    #     state = new_state    #    #     if t % 200 == 0:    #         fig = plt.figure(figsize=(20, 5))    #         plt.suptitle(r'$\nu_{D1}$ = %.2f Hz, $\nu_{D2}$ = %.2f Hz' % (20.0, 20.0), fontsize=20)    #         interval = 500000    #         plt.subplot(1, 2, 1)    #         plt.plot(np.arange(np.maximum(t - interval, 0), t), R[0, t - interval:t], 'k', alpha=1)    #         plt.plot(np.arange(np.maximum(t - interval, 0), t), R[8, t - interval:t], 'r', alpha=1)    #         plt.xlim(np.maximum(t - interval, 0), t)    #         plt.ylim(0, 2)    #         plt.xlabel('Time [s]', fontsize=20)    #         plt.ylabel('Rate [Hz]', fontsize=20)    #         plt.xticks(plt.linspace(np.maximum(t - interval, 0), t, 5),    #                    np.round(plt.linspace(np.maximum(t - interval, 0), t, 5) * dt, 1), fontsize=15)    #         plt.yticks(fontsize=15)    #    #         plt.subplot(1, 2, 2)    #         DT = dominance_time(R[0, 500:t], R[8, 500:t], dt=dt, cutoff=.1)  # dominance duration in seconds    #         counts, bin_edges = np.histogram(DT, bins=20)    #         plt.hist(DT, bins=20, color='r')    #         plt.xlabel('Dominance Duration [s]', fontsize=20)    #         plt.xlim(left=0)    #    #         display.display(plt.gcf())    #         display.clear_output(wait=True)    #         plt.close('all')